# Hugging Face Configuration
HF_API_KEY=your_huggingface_api_key_here
HF_EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
HF_MODEL_NAME=google/gemma-2b-it

# LLM Provider Selection (true/false)
USE_GROQ=false
USE_HUGGINGFACE=true
USE_LOCAL_HF=false  # Set to true to use local HF models instead of API

# Hardware Configuration
USE_GPU=false  # Set to true to use CUDA GPU for local models

# Groq Configuration (if USE_GROQ=true)
GROQ_API_KEY=your_groq_api_key_here

# Ollama Configuration (Fallback)
# No specific env vars needed for Ollama, uses localhost:11434 by default

# Chroma Vector DB Configuration
CHROMA_DB_DIR=./data/chromadb
